{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prerequisites\n",
        "\n",
        "This notebook is made to run on google colab.\n",
        "\n",
        "The first cells allow to set up the environment and to place all data at the right place. "
      ],
      "metadata": {
        "id": "9YhA5ow-q7eG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWrFCUOuKyP9"
      },
      "outputs": [],
      "source": [
        "# Mount drive to have access to some data.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the git repository containing the code and installing correctly the dependencies\n",
        "\n",
        "!git clone https://github.com/tanguy-magne/DirectVoxGo-NPM3D.git\n",
        "%cd DirectVoxGo-NPM3D/DirectVoxGO\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install ninja\n",
        "\n",
        "import torch\n",
        "assert(torch.__version__ == '1.10.0+cu111')\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "\n",
        "!pip install einops\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "weOCZZjxK6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset\n",
        "\n",
        "This part of the code is made to set all dataset in the working environment."
      ],
      "metadata": {
        "id": "zU9XvPc2tDgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_DATA = \"/content/DirectVoxGo-NPM3D/DirectVoxGO/data\" \n",
        "!mkdir -p {PATH_DATA}"
      ],
      "metadata": {
        "id": "y5pFcDcrtN9x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This dataset have to be dowloaded from internet, at this link \n",
        "# https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1\n",
        "# The two .zip files that have to be put on your drive are nerf_synthetic.zip and nerf_llff_data.zip\n",
        "# Then the variable PATH_TO_DATA_DRIVE has to be updated\n",
        "# Note that you can skip this part by commenting it and just not use this dataset\n",
        "\n",
        "PATH_TO_DATA_DRIVE = \"/content/drive/MyDrive/MVA/2.3_npm3d/Project\"\n",
        "datasets = {\n",
        "  \"nerf_synthetic\" : f\"{PATH_TO_DATA_DRIVE}/nerf_synthetic.zip\",\n",
        "  \"nerf_llff_data\" : f\"{PATH_TO_DATA_DRIVE}/nerf_llff_data.zip\"\n",
        "}\n",
        "\n",
        "for dataset, dataset_path in datasets.items():\n",
        "  !unzip -q {dataset_path} -d {PATH_DATA}\n",
        "\n",
        "unwanted_MACOSX = f\"{PATH_DATA}/__MACOSX\"\n",
        "!rm -r {unwanted_MACOSX}"
      ],
      "metadata": {
        "id": "atOhub6TtR_U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The deepvoxels dataset requires the same procedure. The link to dowload it is the following\n",
        "# https://drive.google.com/drive/folders/1ScsRlnzy9Bd_n-xw83SP-0t548v63mPH\n",
        "# You need to download the .zip file synthetic_scenes.zip and place it with the other on your drive\n",
        "\n",
        "PATH_DEEPVOXELS = PATH_DATA + \"/deepvoxels\"\n",
        "PATH_DEEPVOXELS_ON_DRIVE = f\"{PATH_TO_DATA_DRIVE}/synthetic_scenes.zip\"\n",
        "!unzip -q {\"/content/drive/MyDrive/MVA/2.3_npm3d/Project/synthetic_scenes.zip\"} -d {PATH_DEEPVOXELS}"
      ],
      "metadata": {
        "id": "Ydul3XHJtVHl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These datasets are available online and don't require anything to work, you just need to run this code\n",
        "\n",
        "datasets  = {\n",
        "  \"Synthetic_NSVF\" : \"https://dl.fbaipublicfiles.com/nsvf/dataset/Synthetic_NSVF.zip\",\n",
        "  \"BlendedMVS\": \"https://dl.fbaipublicfiles.com/nsvf/dataset/BlendedMVS.zip\",\n",
        "  \"TanksAndTemple\": \"https://dl.fbaipublicfiles.com/nsvf/dataset/TanksAndTemple.zip\",\n",
        "}\n",
        "\n",
        "for dataset, url in datasets.items():\n",
        "  dataset_path = f\"{PATH_DATA}/{dataset}.zip\"\n",
        "  !wget -q -O {dataset_path} {url}\n",
        "  !unzip -q {dataset_path} -d {PATH_DATA}\n",
        "  !rm {dataset_path}"
      ],
      "metadata": {
        "id": "r7OqRRsDtVMm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset : These dataset can be found at this link : https://drive.google.com/drive/folders/1w46Rlg1ZoKXEiu3D6wn-GQT6c0Jcu1Mu?usp=sharing\n",
        "# They should be downloaded and be place on your drive alonside with the NeRF synthetic dataset\n",
        "\n",
        "PATH_NOUNOURS = f\"{PATH_DATA}/nerf_llff_data\"\n",
        "PATH_NOUNOURS_DRIVE = f\"{PATH_TO_DATA_DRIVE}/nounours.zip\"\n",
        "!unzip -q {PATH_NOUNOURS_DRIVE} -d {PATH_NOUNOURS}\n",
        "PATH_PANDA = f\"{PATH_DATA}/nerf_llff_data\"\n",
        "PATH_PANDA_DRIVE = f\"{PATH_TO_DATA_DRIVE}/panda.zip\"\n",
        "!unzip -q {PATH_PANDA_DRIVE} -d {PATH_PANDA}"
      ],
      "metadata": {
        "id": "Kyn4Zob0NX7f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training and Evaluating models\n",
        "\n",
        "To change some parameters, you can simply go to the config files corresponding to the scene you want to train a model on, and change the correponding paramaters. This is how I run all my experiments and ablations study.\n",
        "\n",
        "Here are the paramaters I changed in my experiments, that corresponds to the main contributions of the article :\n",
        "- To change parameters of the coarse stage of the model (only available if not using forward facing scene, which corresponds to LLFF dataset)\n",
        "  - To change the value of $\\alpha^{(init)(c)}$, the initial value of the $\\alpha$ parameter in the coarse stage add this to the config `coarse_model_and_render = dict(alpha_init=value)` with value being the value you want\n",
        "  - To use or not the view-count-based learning rate, add this to the config `coarse_train = dict(pervoxel_lr=True)`. You can change the value to False to not use this feature. \n",
        "- To change parameters of the fine stage of the model :\n",
        "  - To change the value of $\\alpha^{(init)(f)}$, the initial value of the $\\alpha$ parameter in the fine stage add this to the config `fine_model_and_render = dict(alpha_init=value)` with value being the value you want\n",
        "  - To use or not the view-count-based learning rate, add this to the config `fine_train = dict(pervoxel_lr=True)`. You can change the value to False to not use this feature. \n",
        "- To use pre-activation you need to go in the code :\n",
        "  - If working with LLFF dataset go to `DirectVocGO/lib/dmpigo.py` file. Then comment lines 227-229 and uncomment lines 223-225\n",
        "  - Otherwise go to `DirectVocGO/lib/dvgo.py` file. Then comment lines 314-316 and uncomment lines 318-320.\n"
      ],
      "metadata": {
        "id": "GmGNROkstlDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line of command allow to train a model\n",
        "!python run.py --config configs/llff/nounours.py --render_test --eval_ssim --eval_lpips_vgg --eval_lpips_alex"
      ],
      "metadata": {
        "id": "PnWHvqN_UjlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line of command allow to evaluate a model, without running the training again \n",
        "!python run.py --config configs/llff/nounours.py --render_only --render_test --render_video --eval_ssim --eval_lpips_vgg"
      ],
      "metadata": {
        "id": "X51junkCCkXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}